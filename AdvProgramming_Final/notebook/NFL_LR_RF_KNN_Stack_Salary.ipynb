{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81b5ff5-0059-4d3b-96e9-da93682e8849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T05:32:07.613777Z",
     "start_time": "2025-11-27T05:32:07.490448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Proyecto: /Users/mariajosezuniga/Desktop/OIM_7502_classwork/AdvProgramming_Final\n",
      "ðŸ“Š Datos: /Users/mariajosezuniga/Desktop/OIM_7502_classwork/AdvProgramming_Final/data\n",
      "ðŸ’¾ Output: /Users/mariajosezuniga/Desktop/OIM_7502_classwork/AdvProgramming_Final/data\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "NFL Game Prediction and Team Rankings Analysis\n",
    "===============================================\n",
    "This script predicts NFL game outcomes using historical data (2022-2023)\n",
    "and evaluates performance on 2024 season data using multiple ML models:\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Random Forest\n",
    "- Stacked Ensemble\n",
    "\n",
    "It generates team rankings, enriches them with salary and standings data,\n",
    "and produces comprehensive visualizations and correlation analyses.\n",
    "\n",
    "Author: Data Analysis Team\n",
    "Date: 2024\n",
    "\"\"\"\n",
    "\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CONSTANTS AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Detectar ubicaciÃ³n del notebook\n",
    "import os\n",
    "\n",
    "if '__file__' in globals():\n",
    "    # Si corre como script .py\n",
    "    NOTEBOOK_DIR = Path(__file__).parent\n",
    "else:\n",
    "    # Si corre en Jupyter\n",
    "    NOTEBOOK_DIR = Path(os.getcwd())\n",
    "    # Ajustar si getcwd() no es la carpeta notebook/\n",
    "    if NOTEBOOK_DIR.name != 'notebook':\n",
    "        NOTEBOOK_DIR = NOTEBOOK_DIR / 'notebook'\n",
    "\n",
    "# Subir a AdvProgramming_Final/\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"ðŸ“‚ Proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"ðŸ“Š Datos: {DATA_DIR}\")\n",
    "print(f\"ðŸ’¾ Output: {OUTPUT_DIR}\")\n",
    "\n",
    "TEAM_NAME_MAP = {\n",
    "    'KC Chiefs': 'KC', 'ATL Falcons': 'ATL', 'BAL Ravens': 'BAL',\n",
    "    'LAR Rams': 'LA', 'CHI Bears': 'CHI', 'IND Colts': 'IND',\n",
    "    'WAS Command': 'WAS', 'CIN Bengals': 'CIN', 'LAC Chargers': 'LAC',\n",
    "    'PIT Steelers': 'PIT', 'HOU Texans': 'HOU', 'BUF Bills': 'BUF',\n",
    "    'SEA Seahawks': 'SEA', 'PHI Eagles': 'PHI', 'ARI Cardinals': 'ARI',\n",
    "    'MIN Vikings': 'MIN', 'NYJ Jets': 'NYJ', 'GB Packers': 'GB',\n",
    "    'DET Lions': 'DET', 'DEN Broncos': 'DEN', 'CAR Panthers': 'CAR',\n",
    "    'NO Saints': 'NO', 'JAX Jaguars': 'JAX', 'TEN Titans': 'TEN',\n",
    "    'TB Buccaneers': 'TB', 'MIA Dolphins': 'MIA', 'SF 49ers': 'SF',\n",
    "    'CLE Browns': 'CLE', 'NE Patriots': 'NE', 'NYG Giants': 'NYG',\n",
    "    'LV Raiders': 'LV'\n",
    "}\n",
    "\n",
    "STAT_COLS = [\n",
    "    'passing_yards', 'passing_tds', 'passing_interceptions', 'passing_epa',\n",
    "    'rushing_yards', 'rushing_tds', 'rushing_epa',\n",
    "    'receiving_yards', 'receiving_tds', 'receiving_epa',\n",
    "    'def_sacks', 'def_interceptions', 'def_tds',\n",
    "    'turnovers', 'penalties', 'penalty_yards'\n",
    "]\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_prepare_data(dict_path, games_path):\n",
    "    \"\"\"\n",
    "    Load NFL data dictionary and games data, then apply type conversions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_path : Path\n",
    "        Path to the data dictionary Excel file\n",
    "    games_path : Path\n",
    "        Path to the games Excel file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Preprocessed games DataFrame with correct column types\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"LOADING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    dict_df = pd.read_excel(dict_path)\n",
    "    games_df = pd.read_excel(games_path)\n",
    "    \n",
    "    # Normalize column names\n",
    "    dict_df.columns = dict_df.columns.str.lower().str.strip()\n",
    "    games_df.columns = games_df.columns.str.lower().str.strip()\n",
    "    \n",
    "    # Build type mapping\n",
    "    type_map = dict(zip(dict_df['field'], dict_df['type']))\n",
    "    \n",
    "    # Convert types\n",
    "    for col, dtype in type_map.items():\n",
    "        if col not in games_df.columns:\n",
    "            continue\n",
    "        if \"numeric\" in dtype.lower():\n",
    "            games_df[col] = pd.to_numeric(games_df[col], errors='coerce')\n",
    "        elif \"character\" in dtype.lower() or \"categorical\" in dtype.lower():\n",
    "            games_df[col] = games_df[col].astype(\"string\")\n",
    "    \n",
    "    print(f\"Columns: {len(games_df.columns)} | Rows: {len(games_df)}\")\n",
    "    print(\"Seasons available:\", games_df['season'].unique())\n",
    "    print(\"Season types:\", games_df['season_type'].unique())\n",
    "    \n",
    "    return games_df\n",
    "\n",
    "\n",
    "def create_home_away_perspective(games_df):\n",
    "    \"\"\"\n",
    "    Expand games data to create both home and away team perspectives.\n",
    "    Each game becomes two rows, one from each team's viewpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    games_df : pd.DataFrame\n",
    "        Original games data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Expanded DataFrame with team, opponent, points_for, points_against\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING TEAM PERSPECTIVES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Home perspective\n",
    "    home_games = games_df.copy()\n",
    "    home_games['team'] = home_games['home_team']\n",
    "    home_games['opponent_team'] = home_games['away_team']\n",
    "    home_games['points_for'] = home_games['home_score']\n",
    "    home_games['points_against'] = home_games['away_score']\n",
    "    \n",
    "    # Away perspective\n",
    "    away_games = games_df.copy()\n",
    "    away_games['team'] = away_games['away_team']\n",
    "    away_games['opponent_team'] = away_games['home_team']\n",
    "    away_games['points_for'] = away_games['away_score']\n",
    "    away_games['points_against'] = away_games['home_score']\n",
    "    \n",
    "    # Combine\n",
    "    combined_df = pd.concat([home_games, away_games], ignore_index=True)\n",
    "    combined_df['win'] = (\n",
    "        combined_df['points_for'] > combined_df['points_against']\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Create turnovers feature\n",
    "    combined_df['turnovers'] = (\n",
    "        combined_df['passing_interceptions'].fillna(0) +\n",
    "        combined_df['sack_fumbles_lost'].fillna(0) +\n",
    "        combined_df['rushing_fumbles_lost'].fillna(0) +\n",
    "        combined_df['receiving_fumbles_lost'].fillna(0)\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset expanded to {len(combined_df)} rows (2 per game)\")\n",
    "    print(f\"Average turnovers per game: {combined_df['turnovers'].mean():.2f}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def create_team_season_stats(df, stat_cols):\n",
    "    \"\"\"\n",
    "    Create rolling historical averages for each team within each season.\n",
    "    Uses expanding window with shift to avoid data leakage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Games data with team perspective\n",
    "    stat_cols : list\n",
    "        List of statistical columns to aggregate\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added historical average columns\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING HISTORICAL FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = df.sort_values(['team', 'season', 'week']).reset_index(drop=True)\n",
    "    \n",
    "    # Create rolling averages for each stat\n",
    "    for col in stat_cols:\n",
    "        df[f'{col}_team_avg'] = (\n",
    "            df.groupby(['team', 'season'])[col]\n",
    "            .expanding()\n",
    "            .mean()\n",
    "            .reset_index(level=[0, 1], drop=True)\n",
    "            .shift(1)\n",
    "        )\n",
    "    \n",
    "    # Create win percentage\n",
    "    df['win_pct'] = (\n",
    "        df.groupby(['team', 'season'])['win']\n",
    "        .expanding()\n",
    "        .mean()\n",
    "        .reset_index(level=[0, 1], drop=True)\n",
    "        .shift(1)\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values\n",
    "    df['win_pct'] = df['win_pct'].fillna(0.5)\n",
    "    for col in stat_cols:\n",
    "        avg_col = f'{col}_team_avg'\n",
    "        df[avg_col] = df[avg_col].fillna(df[avg_col].median())\n",
    "    \n",
    "    print(f\"Created {len(stat_cols)} historical average features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_opponent_features(df):\n",
    "    \"\"\"\n",
    "    Add opponent's historical statistics to each game record.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with team historical features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added opponent features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ADDING OPPONENT FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    feature_cols = [c for c in df.columns if '_team_avg' in c or c == 'win_pct']\n",
    "    print(f\"Merging {len(feature_cols)} opponent features...\")\n",
    "    \n",
    "    # Create opponent dataframe\n",
    "    opp = df[['season', 'week', 'team'] + feature_cols].copy()\n",
    "    rename_dict = {'team': 'opponent_team'}\n",
    "    rename_dict.update({col: f'opp_{col}' for col in feature_cols})\n",
    "    opp = opp.rename(columns=rename_dict)\n",
    "    \n",
    "    # Merge opponent features\n",
    "    df = df.merge(opp, on=['season', 'week', 'opponent_team'], how='left')\n",
    "    \n",
    "    # Fill missing opponent features with median\n",
    "    for col in feature_cols:\n",
    "        opp_col = f'opp_{col}'\n",
    "        if opp_col in df.columns:\n",
    "            df[opp_col] = df[opp_col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_relative_features(df):\n",
    "    \"\"\"\n",
    "    Create relative features (team vs opponent differences).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with team and opponent features\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added relative features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING RELATIVE FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    count = 0\n",
    "    for col in [c for c in df.columns if '_team_avg' in c]:\n",
    "        opp_col = f\"opp_{col}\"\n",
    "        if opp_col in df.columns:\n",
    "            df[f'rel_{col}'] = df[col] - df[opp_col]\n",
    "            count += 1\n",
    "    \n",
    "    df['is_home'] = (df['team'] == df['home_team']).astype(int)\n",
    "    df['is_playoff'] = (df['season_type'] == 'POST').astype(int)\n",
    "    \n",
    "    print(f\"Created {count} relative features plus is_home and is_playoff\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_train_test(df, train_seasons, test_season):\n",
    "    \"\"\"\n",
    "    Split data into training and test sets by season.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Full games DataFrame\n",
    "    train_seasons : list\n",
    "        List of seasons for training\n",
    "    test_season : str\n",
    "        Season for testing\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (train_df, test_df)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SPLITTING TRAIN/TEST DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df['season'] = df['season'].astype(str).str.strip()\n",
    "    \n",
    "    train_df = df[\n",
    "        (df['season'].isin(train_seasons)) & \n",
    "        (df['season_type'] == 'REG')\n",
    "    ].copy()\n",
    "    \n",
    "    test_df = df[\n",
    "        (df['season'] == test_season) & \n",
    "        (df['season_type'] == 'REG')\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)} ({', '.join(train_seasons)})\")\n",
    "    print(f\"Test samples: {len(test_df)} ({test_season})\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MODEL TRAINING AND EVALUATION\n",
    "# ==============================================================================\n",
    "\n",
    "def prepare_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Extract feature columns and prepare X, y for training and testing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_df : pd.DataFrame\n",
    "        Training data\n",
    "    test_df : pd.DataFrame\n",
    "        Test data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (X_train, y_train, X_test, y_test, feature_cols, scaler)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PREPARING FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    feature_cols = (\n",
    "        [c for c in train_df.columns if c.startswith('rel_')] +\n",
    "        ['win_pct', 'opp_win_pct', 'is_home', 'is_playoff']\n",
    "    )\n",
    "    feature_cols = [c for c in feature_cols if c in train_df.columns]\n",
    "    \n",
    "    X_train = train_df[feature_cols].fillna(0)\n",
    "    y_train = train_df['win']\n",
    "    X_test = test_df[feature_cols].fillna(0)\n",
    "    y_test = test_df['win']\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Features used: {len(feature_cols)}\")\n",
    "    print(\"Sample features:\", feature_cols[:5])\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test, feature_cols, scaler\n",
    "\n",
    "\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate Logistic Regression model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : np.ndarray\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (model, y_pred, y_proba, accuracy)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING LOGISTIC REGRESSION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, cv=5, scoring='accuracy'\n",
    "    )\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} \"\n",
    "          f\"(+/- {cv_scores.std():.3f})\")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, y_pred, y_proba, accuracy\n",
    "\n",
    "\n",
    "def train_knn(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate K-Nearest Neighbors model with GridSearchCV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : np.ndarray\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (best_model, y_pred, y_proba, accuracy, best_params)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING KNN WITH GRID SEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2]\n",
    "    }\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(\n",
    "        knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=0\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model, y_pred, y_proba, accuracy, grid_search.best_params_\n",
    "\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate Random Forest model with GridSearchCV.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : np.ndarray\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (best_model, y_pred, y_proba, accuracy, best_params)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING RANDOM FOREST WITH GRID SEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 300],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "    grid_search = GridSearchCV(\n",
    "        rf, param_grid, cv=4, scoring='accuracy', n_jobs=-1, verbose=0\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return best_model, y_pred, y_proba, accuracy, grid_search.best_params_\n",
    "\n",
    "\n",
    "def train_stacked_model(models_dict, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Train a stacked ensemble model using base model predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models_dict : dict\n",
    "        Dictionary of trained models {'lr': model, 'knn': model, 'rf': model}\n",
    "    X_train : np.ndarray\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : np.ndarray\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (stacker, y_pred, y_proba, accuracy)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING STACKED ENSEMBLE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Generate meta-features from base models\n",
    "    train_meta = pd.DataFrame({\n",
    "        'lr_prob': models_dict['lr'].predict_proba(X_train)[:, 1],\n",
    "        'knn_prob': models_dict['knn'].predict_proba(X_train)[:, 1],\n",
    "        'rf_prob': models_dict['rf'].predict_proba(X_train)[:, 1]\n",
    "    })\n",
    "    \n",
    "    test_meta = pd.DataFrame({\n",
    "        'lr_prob': models_dict['lr'].predict_proba(X_test)[:, 1],\n",
    "        'knn_prob': models_dict['knn'].predict_proba(X_test)[:, 1],\n",
    "        'rf_prob': models_dict['rf'].predict_proba(X_test)[:, 1]\n",
    "    })\n",
    "    \n",
    "    # Train meta-model\n",
    "    stacker = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    stacker.fit(train_meta, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = stacker.predict(test_meta)\n",
    "    y_proba = stacker.predict_proba(test_meta)[:, 1]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Stacked model accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Show meta-model coefficients\n",
    "    coef_df = pd.DataFrame({\n",
    "        'base_model': ['lr_prob', 'knn_prob', 'rf_prob'],\n",
    "        'coefficient': stacker.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nMeta-Model Coefficients:\")\n",
    "    print(coef_df.to_string(index=False))\n",
    "    \n",
    "    return stacker, y_pred, y_proba, accuracy\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ANALYSIS AND RANKINGS\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_permutation_importance(model, X_test, y_test, feature_cols, \n",
    "                                   model_name):\n",
    "    \"\"\"\n",
    "    Compute permutation importance for a model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn model\n",
    "        Trained model\n",
    "    X_test : np.ndarray\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "    feature_cols : list\n",
    "        List of feature names\n",
    "    model_name : str\n",
    "        Name of the model for display\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with feature importances\n",
    "    \"\"\"\n",
    "    print(f\"\\nComputing permutation importance for {model_name}...\")\n",
    "    \n",
    "    perm = permutation_importance(\n",
    "        model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'perm_importance_mean': perm.importances_mean,\n",
    "        'perm_importance_std': perm.importances_std\n",
    "    }).sort_values('perm_importance_mean', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def compute_team_rankings(test_df, y_proba, model_name):\n",
    "    \"\"\"\n",
    "    Compute team rankings based on model predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_df : pd.DataFrame\n",
    "        Test data with team information\n",
    "    y_proba : np.ndarray\n",
    "        Predicted win probabilities\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with team rankings\n",
    "    \"\"\"\n",
    "    pred_df = test_df[['team', 'win']].copy()\n",
    "    pred_df['predicted_prob'] = y_proba\n",
    "    \n",
    "    # Team strength (average predicted win probability)\n",
    "    team_strength = (\n",
    "        pred_df.groupby('team')['predicted_prob']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'predicted_prob': 'avg_pred_prob'})\n",
    "    )\n",
    "    \n",
    "    # Actual win rate\n",
    "    actual_win_rate = (\n",
    "        pred_df.groupby('team')['win']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'win': 'actual_win_pct'})\n",
    "    )\n",
    "    \n",
    "    # Combine and rank\n",
    "    rankings = team_strength.merge(actual_win_rate, on='team', how='left')\n",
    "    rankings = rankings.sort_values(\n",
    "        'avg_pred_prob', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "    rankings['predicted_rank'] = rankings.index + 1\n",
    "    rankings['actual_rank'] = rankings['actual_win_pct'].rank(ascending=False)\n",
    "    rankings['rank_diff'] = rankings['predicted_rank'] - rankings['actual_rank']\n",
    "    rankings['model'] = model_name\n",
    "    \n",
    "    # Compute correlation\n",
    "    corr = rankings['avg_pred_prob'].corr(rankings['actual_win_pct'])\n",
    "    print(f\"\\n{model_name} - Correlation between predicted and actual: {corr:.3f}\")\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "\n",
    "def enrich_rankings_with_external_data(rankings_df, salary_df, \n",
    "                                       standings_df, team_map):\n",
    "    \"\"\"\n",
    "    Enrich rankings with salary and standings data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rankings_df : pd.DataFrame\n",
    "        Team rankings DataFrame\n",
    "    salary_df : pd.DataFrame\n",
    "        Salary data by team\n",
    "    standings_df : pd.DataFrame\n",
    "        Official standings data\n",
    "    team_map : dict\n",
    "        Mapping of team names to abbreviations\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Enriched rankings DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ENRICHING RANKINGS WITH EXTERNAL DATA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Map team names to abbreviations\n",
    "    salary_df['team_abbrev'] = salary_df['team'].map(team_map)\n",
    "    \n",
    "    # Merge salary data\n",
    "    salary_cols = [\n",
    "        'team_abbrev', 'median_salary', 'avg_salary',\n",
    "        'total_salary', 'num_players'\n",
    "    ]\n",
    "    enriched = rankings_df.merge(\n",
    "        salary_df[salary_cols],\n",
    "        left_on='team',\n",
    "        right_on='team_abbrev',\n",
    "        how='left'\n",
    "    ).drop(columns=['team_abbrev'])\n",
    "    \n",
    "    # Merge standings data\n",
    "    standings_cols = [\n",
    "        'team', 'wins', 'losses', 'win_pct', 'points_scored',\n",
    "        'points_allowed', 'point_diff', 'games_played'\n",
    "    ]\n",
    "    enriched = enriched.merge(\n",
    "        standings_df[standings_cols],\n",
    "        on='team',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(\"Enrichment complete\")\n",
    "    \n",
    "    return enriched\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_confusion_matrices(results_dict, y_test):\n",
    "    \"\"\"\n",
    "    Plot side-by-side confusion matrices for all models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dict : dict\n",
    "        Dictionary of model results {'model_name': {'y_pred': ...}, ...}\n",
    "    y_test : pd.Series\n",
    "        True test labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING CONFUSION MATRICES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(results_dict), figsize=(4*len(results_dict), 4))\n",
    "    \n",
    "    if len(results_dict) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (model_name, results) in zip(axes, results_dict.items()):\n",
    "        cm = confusion_matrix(y_test, results['y_pred'], labels=[0, 1])\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax,\n",
    "            xticklabels=[\"Pred Loss\", \"Pred Win\"],\n",
    "            yticklabels=[\"True Loss\", \"True Win\"]\n",
    "        )\n",
    "        ax.set_ylabel(\"Actual\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_title(f\"{model_name} Confusion Matrix\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predicted_vs_actual(enriched_rankings):\n",
    "    \"\"\"\n",
    "    Plot predicted vs actual win percentage for each model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    enriched_rankings : pd.DataFrame\n",
    "        Enriched rankings with all models\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING PREDICTED VS ACTUAL PLOTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    models = enriched_rankings['model'].unique()\n",
    "    \n",
    "    for model_name in models:\n",
    "        df_model = enriched_rankings[enriched_rankings['model'] == model_name]\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_model, x='avg_pred_prob', y='actual_win_pct',\n",
    "            hue='team', s=100, alpha=0.7\n",
    "        )\n",
    "        sns.regplot(\n",
    "            data=df_model, x='avg_pred_prob', y='actual_win_pct',\n",
    "            scatter=False, color='black', line_kws={'linewidth': 2}\n",
    "        )\n",
    "        \n",
    "        # Add correlation text\n",
    "        r, _ = pearsonr(df_model['avg_pred_prob'], df_model['actual_win_pct'])\n",
    "        plt.text(\n",
    "            0.05, 0.95, f\"Pearson r = {r:.3f}\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\")\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"{model_name}: Predicted vs Actual Win %\")\n",
    "        plt.xlabel(\"Predicted Win %\")\n",
    "        plt.ylabel(\"Actual Win %\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_salary_analysis(enriched_rankings):\n",
    "    \"\"\"\n",
    "    Plot salary vs win percentage analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    enriched_rankings : pd.DataFrame\n",
    "        Enriched rankings with salary data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING SALARY ANALYSIS PLOTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    clean_df = enriched_rankings.dropna(\n",
    "        subset=['median_salary', 'avg_pred_prob', 'actual_win_pct']\n",
    "    )\n",
    "    \n",
    "    models = clean_df['model'].unique()\n",
    "    \n",
    "    # Predicted Win % vs Median Salary\n",
    "    for model_name in models:\n",
    "        df_model = clean_df[clean_df['model'] == model_name]\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_model, x='median_salary', y='avg_pred_prob',\n",
    "            hue='team', s=100, alpha=0.7\n",
    "        )\n",
    "        sns.regplot(\n",
    "            data=df_model, x='median_salary', y='avg_pred_prob',\n",
    "            scatter=False, color='black', line_kws={'linewidth': 2}\n",
    "        )\n",
    "        \n",
    "        r, _ = pearsonr(df_model['median_salary'], df_model['avg_pred_prob'])\n",
    "        plt.text(\n",
    "            0.05, 0.95, f\"Pearson r = {r:.3f}\",\n",
    "            transform=plt.gca().transAxes,\n",
    "            fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\")\n",
    "        )\n",
    "        \n",
    "        min_sal = df_model['median_salary'].min()\n",
    "        max_sal = df_model['median_salary'].max()\n",
    "        plt.text(\n",
    "            0.05, 0.85,\n",
    "            f\"Salary Range: ${min_sal:,.0f} - ${max_sal:,.0f}\",\n",
    "            transform=plt.gca().transAxes, fontsize=11,\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\")\n",
    "        )\n",
    "        \n",
    "        plt.title(f\"{model_name}: Predicted Win % vs Median Salary\")\n",
    "        plt.xlabel(\"Median Salary ($)\")\n",
    "        plt.ylabel(\"Predicted Win %\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Actual Win % vs Median Salary (all teams)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        data=clean_df, x='median_salary', y='actual_win_pct',\n",
    "        hue='team', s=100, alpha=0.7\n",
    "    )\n",
    "    sns.regplot(\n",
    "        data=clean_df, x='median_salary', y='actual_win_pct',\n",
    "        scatter=False, color='black', line_kws={'linewidth': 2}\n",
    "    )\n",
    "    \n",
    "    r, _ = pearsonr(clean_df['median_salary'], clean_df['actual_win_pct'])\n",
    "    plt.text(\n",
    "        0.05, 0.95, f\"Pearson r = {r:.3f}\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12, verticalalignment='top',\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\")\n",
    "    )\n",
    "    \n",
    "    min_sal = clean_df['median_salary'].min()\n",
    "    max_sal = clean_df['median_salary'].max()\n",
    "    plt.text(\n",
    "        0.05, 0.85,\n",
    "        f\"Salary Range: ${min_sal:,.0f} - ${max_sal:,.0f}\",\n",
    "        transform=plt.gca().transAxes, fontsize=11,\n",
    "        bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\")\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Actual Win % vs Median Salary\")\n",
    "    plt.xlabel(\"Median Salary ($)\")\n",
    "    plt.ylabel(\"Actual Win %\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rankings_position_bars(position_table, models):\n",
    "    \"\"\"\n",
    "    Plot bar charts comparing predicted vs actual rankings for each model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    position_table : pd.DataFrame\n",
    "        Table with Position, Actual, and model columns\n",
    "    models : list\n",
    "        List of model names to plot\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING RANKING POSITION BAR CHARTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    actual_order = position_table['Actual'].tolist()\n",
    "    x = np.arange(len(actual_order))\n",
    "    \n",
    "    for model_name in models:\n",
    "        predicted_order = position_table[model_name].tolist()\n",
    "        predicted_rank_map = {\n",
    "            team: rank for rank, team in enumerate(predicted_order, start=1)\n",
    "        }\n",
    "        \n",
    "        actual_ranks = position_table['Position'].tolist()\n",
    "        predicted_ranks = [\n",
    "            predicted_rank_map.get(team, len(actual_order) + 1)\n",
    "            for team in actual_order\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        bar_width = 0.6\n",
    "        \n",
    "        # Actual ranks (solid)\n",
    "        plt.bar(\n",
    "            x, actual_ranks, width=bar_width, color='orange',\n",
    "            label='Actual Rank'\n",
    "        )\n",
    "        # Predicted ranks (transparent)\n",
    "        plt.bar(\n",
    "            x, predicted_ranks, width=bar_width, color='steelblue',\n",
    "            alpha=0.5, label=f'{model_name} Predicted Rank'\n",
    "        )\n",
    "        \n",
    "        plt.xticks(x, actual_order, rotation=45, ha='right')\n",
    "        plt.xlabel('Teams (ordered by actual standings)')\n",
    "        plt.ylabel('Rank Position')\n",
    "        plt.title(f'{model_name}: Predicted vs Actual Rank')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def print_correlation_analysis(enriched_rankings):\n",
    "    \"\"\"\n",
    "    Print comprehensive correlation analysis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    enriched_rankings : pd.DataFrame\n",
    "        Enriched rankings DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    corr_vars = [\n",
    "        'median_salary', 'avg_salary', 'total_salary',\n",
    "        'actual_win_pct', 'avg_pred_prob', 'point_diff'\n",
    "    ]\n",
    "    \n",
    "    clean_df = enriched_rankings.dropna(subset=corr_vars)\n",
    "    \n",
    "    print(\"\\nPearson Correlation Matrix:\")\n",
    "    print(clean_df[corr_vars].corr(method='pearson').round(3))\n",
    "    \n",
    "    print(\"\\nSpearman Correlation Matrix:\")\n",
    "    print(clean_df[corr_vars].corr(method='spearman').round(3))\n",
    "\n",
    "\n",
    "def create_position_table(rankings_dict, standings_df):\n",
    "    \"\"\"\n",
    "    Create a position table comparing all model rankings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rankings_dict : dict\n",
    "        Dictionary of rankings DataFrames by model name\n",
    "    standings_df : pd.DataFrame\n",
    "        Official standings data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Position table with actual and predicted rankings\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CREATING POSITION TABLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sort by actual standings\n",
    "    actual_order = standings_df.sort_values(\n",
    "        'win_pct', ascending=False\n",
    "    )['team'].tolist()\n",
    "    \n",
    "    position_data = {'Position': range(1, len(actual_order) + 1)}\n",
    "    position_data['Actual'] = actual_order\n",
    "    \n",
    "    # Add each model's predicted order\n",
    "    for model_name, rankings_df in rankings_dict.items():\n",
    "        model_order = rankings_df.sort_values(\n",
    "            'predicted_rank'\n",
    "        )['team'].tolist()\n",
    "        position_data[model_name] = model_order\n",
    "    \n",
    "    position_table = pd.DataFrame(position_data)\n",
    "    \n",
    "    print(\"\\nTeam Rankings by Position:\")\n",
    "    print(position_table.to_string(index=False))\n",
    "    \n",
    "    return position_table\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline for NFL prediction analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"NFL GAME PREDICTION AND TEAM RANKINGS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. LOAD AND PREPARE DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    games_df = load_and_prepare_data(\n",
    "        DATA_DIR / \"nfl_dictionaries_2.xlsx\",\n",
    "        DATA_DIR / \"global_team_games_2.xlsx\"\n",
    "    )\n",
    "    \n",
    "    games_df = create_home_away_perspective(games_df)\n",
    "    games_df = create_team_season_stats(games_df, STAT_COLS)\n",
    "    games_df = add_opponent_features(games_df)\n",
    "    games_df = create_relative_features(games_df)\n",
    "    \n",
    "    train_df, test_df = split_train_test(\n",
    "        games_df,\n",
    "        train_seasons=['2022', '2023'],\n",
    "        test_season='2024'\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. PREPARE FEATURES AND TRAIN BASE MODELS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, feature_cols, scaler = prepare_features(\n",
    "        train_df, test_df\n",
    "    )\n",
    "    \n",
    "    # Train Logistic Regression\n",
    "    lr_model, lr_pred, lr_proba, lr_acc = train_logistic_regression(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # Train KNN\n",
    "    knn_model, knn_pred, knn_proba, knn_acc, knn_params = train_knn(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf_model, rf_pred, rf_proba, rf_acc, rf_params = train_random_forest(\n",
    "        X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. TRAIN STACKED ENSEMBLE MODEL\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    models_dict = {'lr': lr_model, 'knn': knn_model, 'rf': rf_model}\n",
    "    stacker, stack_pred, stack_proba, stack_acc = train_stacked_model(\n",
    "        models_dict, X_train, y_train, X_test, y_test\n",
    "    )\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. COMPUTE RANKINGS FOR ALL MODELS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    rank_lr = compute_team_rankings(test_df, lr_proba, \"Logistic\")\n",
    "    rank_knn = compute_team_rankings(test_df, knn_proba, \"KNN\")\n",
    "    rank_rf = compute_team_rankings(test_df, rf_proba, \"RandomForest\")\n",
    "    rank_stack = compute_team_rankings(test_df, stack_proba, \"Stacked\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. ENRICH RANKINGS WITH EXTERNAL DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Load external data\n",
    "    salary_df = pd.read_csv(\n",
    "        DATA_DIR / \"nfl_salary_by_team_2024_complete.csv\"\n",
    "    )\n",
    "    standings_df = pd.read_csv(DATA_DIR / \"nfl_standings_2024.csv\")\n",
    "    \n",
    "    # Enrich each ranking\n",
    "    rank_lr_enriched = enrich_rankings_with_external_data(\n",
    "        rank_lr, salary_df, standings_df, TEAM_NAME_MAP\n",
    "    )\n",
    "    rank_knn_enriched = enrich_rankings_with_external_data(\n",
    "        rank_knn, salary_df, standings_df, TEAM_NAME_MAP\n",
    "    )\n",
    "    rank_rf_enriched = enrich_rankings_with_external_data(\n",
    "        rank_rf, salary_df, standings_df, TEAM_NAME_MAP\n",
    "    )\n",
    "    rank_stack_enriched = enrich_rankings_with_external_data(\n",
    "        rank_stack, salary_df, standings_df, TEAM_NAME_MAP\n",
    "    )\n",
    "    \n",
    "    # Combine all rankings\n",
    "    all_rankings = pd.concat([\n",
    "        rank_lr_enriched, rank_knn_enriched,\n",
    "        rank_rf_enriched, rank_stack_enriched\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 6. FEATURE IMPORTANCE ANALYSIS\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    lr_importance = compute_permutation_importance(\n",
    "        lr_model, X_test, y_test, feature_cols, \"Logistic Regression\"\n",
    "    )\n",
    "    knn_importance = compute_permutation_importance(\n",
    "        knn_model, X_test, y_test, feature_cols, \"KNN\"\n",
    "    )\n",
    "    rf_importance = compute_permutation_importance(\n",
    "        rf_model, X_test, y_test, feature_cols, \"Random Forest\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 15 FEATURES BY PERMUTATION IMPORTANCE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nLogistic Regression:\")\n",
    "    print(lr_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Guardar CSVs (DENTRO de la funciÃ³n main)\n",
    "    rank_lr.to_csv(OUTPUT_DIR / \"rankings_lr.csv\", index=False)\n",
    "    rank_knn.to_csv(OUTPUT_DIR / \"rankings_knn.csv\", index=False)\n",
    "    rank_rf.to_csv(OUTPUT_DIR / \"rankings_rf.csv\", index=False)\n",
    "    rank_stack.to_csv(OUTPUT_DIR / \"rankings_stack.csv\", index=False)\n",
    "    all_rankings.to_csv(OUTPUT_DIR / \"all_rankings_enriched.csv\", index=False)\n",
    "    \n",
    "    print(\"\\nâœ… CSVs SAVED SUCCESSFULLY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02999cfe-9435-4ca2-83a8-62c081422124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NFL GAME PREDICTION AND TEAM RANKINGS ANALYSIS\n",
      "============================================================\n",
      "============================================================\n",
      "LOADING DATA\n",
      "============================================================\n",
      "Columns: 113 | Rows: 1708\n",
      "Seasons available: <StringArray>\n",
      "['2022', '2023', '2024']\n",
      "Length: 3, dtype: string\n",
      "Season types: <StringArray>\n",
      "['REG', 'POST']\n",
      "Length: 2, dtype: string\n",
      "\n",
      "============================================================\n",
      "CREATING TEAM PERSPECTIVES\n",
      "============================================================\n",
      "Dataset expanded to 3416 rows (2 per game)\n",
      "Average turnovers per game: 1.20\n",
      "\n",
      "============================================================\n",
      "CREATING HISTORICAL FEATURES\n",
      "============================================================\n",
      "Created 16 historical average features\n",
      "\n",
      "============================================================\n",
      "ADDING OPPONENT FEATURES\n",
      "============================================================\n",
      "Merging 17 opponent features...\n",
      "\n",
      "============================================================\n",
      "CREATING RELATIVE FEATURES\n",
      "============================================================\n",
      "Created 16 relative features plus is_home and is_playoff\n",
      "\n",
      "============================================================\n",
      "SPLITTING TRAIN/TEST DATA\n",
      "============================================================\n",
      "Training samples: 4344 (2022, 2023)\n",
      "Test samples: 2176 (2024)\n",
      "\n",
      "============================================================\n",
      "PREPARING FEATURES\n",
      "============================================================\n",
      "Features used: 20\n",
      "Sample features: ['rel_passing_yards_team_avg', 'rel_passing_tds_team_avg', 'rel_passing_interceptions_team_avg', 'rel_passing_epa_team_avg', 'rel_rushing_yards_team_avg']\n",
      "\n",
      "============================================================\n",
      "TRAINING LOGISTIC REGRESSION\n",
      "============================================================\n",
      "Cross-validation accuracy: 0.631 (+/- 0.024)\n",
      "Test accuracy: 0.729\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.73      0.73      1088\n",
      "           1       0.73      0.73      0.73      1088\n",
      "\n",
      "    accuracy                           0.73      2176\n",
      "   macro avg       0.73      0.73      0.73      2176\n",
      "weighted avg       0.73      0.73      0.73      2176\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING KNN WITH GRID SEARCH\n",
      "============================================================\n",
      "Best parameters: {'n_neighbors': 11, 'p': 1, 'weights': 'distance'}\n",
      "Test accuracy: 0.612\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.62      0.61      1088\n",
      "           1       0.61      0.61      0.61      1088\n",
      "\n",
      "    accuracy                           0.61      2176\n",
      "   macro avg       0.61      0.61      0.61      2176\n",
      "weighted avg       0.61      0.61      0.61      2176\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING RANDOM FOREST WITH GRID SEARCH\n",
      "============================================================\n",
      "Best parameters: {'max_depth': 5, 'min_samples_leaf': 2, 'n_estimators': 100}\n",
      "Test accuracy: 0.715\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72      1088\n",
      "           1       0.72      0.71      0.71      1088\n",
      "\n",
      "    accuracy                           0.71      2176\n",
      "   macro avg       0.71      0.71      0.71      2176\n",
      "weighted avg       0.71      0.71      0.71      2176\n",
      "\n",
      "\n",
      "============================================================\n",
      "TRAINING STACKED ENSEMBLE MODEL\n",
      "============================================================\n",
      "Stacked model accuracy: 0.625\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.63      0.63      1088\n",
      "           1       0.63      0.62      0.62      1088\n",
      "\n",
      "    accuracy                           0.62      2176\n",
      "   macro avg       0.63      0.62      0.62      2176\n",
      "weighted avg       0.63      0.62      0.62      2176\n",
      "\n",
      "\n",
      "Meta-Model Coefficients:\n",
      "base_model  coefficient\n",
      "  knn_prob    10.472175\n",
      "   rf_prob     1.005323\n",
      "   lr_prob     0.795190\n",
      "\n",
      "Logistic - Correlation between predicted and actual: 0.815\n",
      "\n",
      "KNN - Correlation between predicted and actual: 0.635\n",
      "\n",
      "RandomForest - Correlation between predicted and actual: 0.843\n",
      "\n",
      "Stacked - Correlation between predicted and actual: 0.666\n",
      "\n",
      "============================================================\n",
      "ENRICHING RANKINGS WITH EXTERNAL DATA\n",
      "============================================================\n",
      "Enrichment complete\n",
      "\n",
      "============================================================\n",
      "ENRICHING RANKINGS WITH EXTERNAL DATA\n",
      "============================================================\n",
      "Enrichment complete\n",
      "\n",
      "============================================================\n",
      "ENRICHING RANKINGS WITH EXTERNAL DATA\n",
      "============================================================\n",
      "Enrichment complete\n",
      "\n",
      "============================================================\n",
      "ENRICHING RANKINGS WITH EXTERNAL DATA\n",
      "============================================================\n",
      "Enrichment complete\n",
      "\n",
      "Computing permutation importance for Logistic Regression...\n",
      "\n",
      "Computing permutation importance for KNN...\n",
      "\n",
      "Computing permutation importance for Random Forest...\n",
      "\n",
      "============================================================\n",
      "TOP 15 FEATURES BY PERMUTATION IMPORTANCE\n",
      "============================================================\n",
      "\n",
      "Logistic Regression:\n",
      "                           feature  perm_importance_mean  perm_importance_std\n",
      "                       opp_win_pct              0.115763             0.006262\n",
      "                           win_pct              0.114706             0.006780\n",
      "          rel_rushing_epa_team_avg              0.046324             0.006469\n",
      "rel_passing_interceptions_team_avg              0.016774             0.003769\n",
      "                           is_home              0.015809             0.004198\n",
      "        rel_receiving_epa_team_avg              0.015074             0.003444\n",
      "            rel_turnovers_team_avg              0.013097             0.003029\n",
      "    rel_def_interceptions_team_avg              0.010110             0.003512\n",
      "            rel_def_sacks_team_avg              0.006664             0.002782\n",
      "      rel_receiving_yards_team_avg              0.006250             0.002834\n",
      "        rel_passing_yards_team_avg              0.006158             0.002751\n",
      "        rel_penalty_yards_team_avg              0.004688             0.001450\n",
      "          rel_passing_tds_team_avg              0.003171             0.001386\n",
      "        rel_receiving_tds_team_avg              0.003171             0.001386\n",
      "            rel_penalties_team_avg              0.003125             0.000735\n",
      "\n",
      "âœ… CSVs SAVED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "main ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaaff5-3dc7-43b7-8854-ca2202ae6306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
